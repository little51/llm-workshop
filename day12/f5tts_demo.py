import torch
import gradio as gr
import soundfile as sf
import tempfile
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from cached_path import cached_path
import json
import numpy as np
from utils_infer_noffmpeg import (
    infer_process,
    load_model,
    load_vocoder,
    preprocess_ref_audio_text
)
from f5_tts.model import DiT

DEFAULT_TTS_MODEL = "F5-TTS_v1"
DEFAULT_TTS_MODEL_CFG = [
    "models/SWivid/F5-TTS_Emilia-ZH-EN/model_1250000.safetensors",
    "models/SWivid/F5-TTS_Emilia-ZH-EN/vocab.txt",
    json.dumps(dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)),
]

# åˆå§‹åŒ–æ¨¡å‹
def load_whisper_model():
    """åŠ è½½Whisperè¯­éŸ³è¯†åˆ«æ¨¡å‹"""
    model_id = "models/openai/whisper-large-v3-turbo"
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_id, dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to("cuda" if torch.cuda.is_available() else "cpu")
    processor = AutoProcessor.from_pretrained(model_id)
    return model, processor

def load_f5tts():
    """åŠ è½½F5-TTSè¯­éŸ³åˆæˆæ¨¡å‹"""
    ckpt_path = str(cached_path(DEFAULT_TTS_MODEL_CFG[0]))
    F5TTS_model_cfg = json.loads(DEFAULT_TTS_MODEL_CFG[2])
    return load_model(DiT, F5TTS_model_cfg, ckpt_path)

# å…¨å±€å˜é‡
whisper_model = None
whisper_processor = None
f5tts_model = None
vocoder = None

def init_models():
    """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
    global whisper_model, whisper_processor, f5tts_model, vocoder
    print("åŠ è½½Whisperè¯­éŸ³è¯†åˆ«æ¨¡å‹...")
    whisper_model, whisper_processor = load_whisper_model()
    print("åŠ è½½F5-TTSè¯­éŸ³åˆæˆæ¨¡å‹...")
    f5tts_model = load_f5tts()
    print("åŠ è½½Vocoder...")
    vocoder = load_vocoder(is_local=True, local_path="models/charactr/vocos-mel-24khz")
    print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")

def transcribe_audio(audio_file):
    """ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘"""
    if not audio_file:
        return ""
    try:
        import librosa
        audio_data, _ = librosa.load(audio_file, sr=16000, mono=True)
        audio_data = audio_data.astype(np.float32) 
        inputs = whisper_processor(
            audio_data,
            sampling_rate=16000,
            return_tensors="pt",
            language="zh",
            task="transcribe"
        )
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model_dtype = next(whisper_model.parameters()).dtype
        processed_inputs = {}
        for key, value in inputs.items():
            if value.dtype == torch.float:  
                processed_inputs[key] = value.to(device=device, dtype=model_dtype)
            else: 
                processed_inputs[key] = value.to(device)
        with torch.no_grad():
            generated_ids = whisper_model.generate(**processed_inputs)
        return whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    except Exception as e:
        print(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        return ""

def synthesize_speech(ref_audio_file, ref_text, gen_text, seed=0, speed=1.0):
    """åˆæˆè¯­éŸ³"""
    if not ref_audio_file or not ref_text.strip() or not gen_text.strip():
        return None, None, "è¯·æä¾›æ‰€æœ‰å¿…è¦è¾“å…¥"
    try:
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(seed)
        # é¢„å¤„ç†å‚è€ƒéŸ³é¢‘å’Œæ–‡æœ¬
        ref_audio, ref_text_processed = preprocess_ref_audio_text(ref_audio_file, ref_text)
        # åˆæˆè¯­éŸ³
        final_wave, final_sample_rate, _ = infer_process(
            ref_audio,
            ref_text_processed,
            gen_text,
            f5tts_model,
            vocoder,
            cross_fade_duration=0.15,
            nfe_step=32,
            speed=speed,
            show_info=print,
            progress=None,
        )
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            temp_path = f.name
            sf.write(temp_path, final_wave, final_sample_rate)
        return temp_path, final_sample_rate, "åˆæˆæˆåŠŸ"
    except Exception as e:
        print(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        return None, None, f"åˆæˆå¤±è´¥: {str(e)}"

def process_full_pipeline(audio_file, gen_text, use_auto_transcribe=True, seed=0, speed=1.0):
    """å®Œæ•´çš„å¤„ç†æµç¨‹ï¼šè¯­éŸ³è¯†åˆ« + è¯­éŸ³åˆæˆ"""
    # æ­¥éª¤1: è¯­éŸ³è¯†åˆ«
    if use_auto_transcribe and audio_file:
        ref_text = transcribe_audio(audio_file)
    else:
        ref_text = ""
    # æ­¥éª¤2: è¯­éŸ³åˆæˆ
    if audio_file and gen_text.strip():
        audio_path, sample_rate, message = synthesize_speech(audio_file, ref_text, gen_text, seed, speed)
        return ref_text, audio_path, message
    else:
        return ref_text, None, "è¯·æä¾›å‚è€ƒéŸ³é¢‘å’Œè¦ç”Ÿæˆçš„æ–‡æœ¬"

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="è¯­éŸ³è¯†åˆ«ä¸åˆæˆç³»ç»Ÿ") as app:
        gr.Markdown("# ğŸ¤ è¯­éŸ³è¯†åˆ«ä¸åˆæˆç³»ç»Ÿ")
        gr.Markdown("ä¸Šä¼ å‚è€ƒéŸ³é¢‘ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è¯†åˆ«è¯­éŸ³å†…å®¹ï¼Œå¹¶æ ¹æ®è¾“å…¥çš„æ–‡æœ¬ç”Ÿæˆæ–°çš„è¯­éŸ³ã€‚")
        with gr.Row():
            with gr.Column():
                # è¾“å…¥éƒ¨åˆ†
                audio_input = gr.Audio(
                    label="ä¸Šä¼ å‚è€ƒéŸ³é¢‘",
                    type="filepath",
                    sources=["upload", "microphone"],
                )
                
                auto_transcribe = gr.Checkbox(
                    label="è‡ªåŠ¨è¯­éŸ³è¯†åˆ«",
                    value=True,
                    info="å‹¾é€‰åè‡ªåŠ¨è¯†åˆ«å‚è€ƒéŸ³é¢‘çš„å†…å®¹"
                )
                
                gen_text_input = gr.Textbox(
                    label="è¾“å…¥è¦ç”Ÿæˆçš„æ–‡æœ¬",
                    placeholder="è¯·è¾“å…¥è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬...",
                    lines=5,
                )
                
                with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                    seed_input = gr.Number(
                        label="éšæœºç§å­",
                        value=0,
                        minimum=0,
                        maximum=2147483647,
                        step=1,
                        info="è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"
                    )
                    
                    speed_slider = gr.Slider(
                        label="è¯­é€Ÿ",
                        minimum=0.5,
                        maximum=2.0,
                        value=1.0,
                        step=0.1,
                        info="è°ƒæ•´è¯­éŸ³çš„æ’­æ”¾é€Ÿåº¦"
                    )
                
                process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                
            with gr.Column():
                # è¾“å‡ºéƒ¨åˆ†
                transcribe_result = gr.Textbox(
                    label="è¯­éŸ³è¯†åˆ«ç»“æœ",
                    interactive=False,
                    lines=3,
                )
                
                audio_output = gr.Audio(
                    label="åˆæˆçš„è¯­éŸ³",
                    autoplay=True,
                )
                
                status_output = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                )
        
        # æŒ‰é’®ç‚¹å‡»äº‹ä»¶
        process_btn.click(
            fn=process_full_pipeline,
            inputs=[
                audio_input,
                gen_text_input,
                auto_transcribe,
                seed_input,
                speed_slider,
            ],
            outputs=[
                transcribe_result,
                audio_output,
                status_output,
            ]
        )
        
        # æ¸…é™¤è¾“å…¥äº‹ä»¶
        audio_input.clear(
            lambda: ["", None, ""],
            outputs=[transcribe_result, audio_output, status_output]
        )
    
    return app

# ä¸»å‡½æ•°
def main():
    print("åˆå§‹åŒ–æ¨¡å‹...")
    init_models()
    print("åˆ›å»ºGradioç•Œé¢...")
    app = create_interface()
    # å¯åŠ¨åº”ç”¨
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        inbrowser=True
    )

if __name__ == "__main__":
    main()